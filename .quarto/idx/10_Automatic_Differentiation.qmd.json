{"title":"Automatic Differentiation","markdown":{"yaml":{"title":"Automatic Differentiation"},"headingText":"Dual Numbers - Forward Mode AD","containsRefs":false,"markdown":"\n\n\nAny reader of this document will be familiar with complex numbers:\n\n$$z = x + iy$$ where $i^2 = -1$\n\nThe concept of [dual numbers](https://en.wikipedia.org/wiki/Dual_number), proposed by Clifford, in 1873, is closely related:\n\n$$z = a + {\\epsilon}b$$ where ${\\epsilon}^2 = 0, \\epsilon \\ne 0$\n\nIt can be proven that for any analytic[^1] function,\n\n[^1]: Any function that is locally given by a convergent power series, such as a Taylor series\n\n$$f(a + {\\epsilon}b) = f(a) + bf'(a){\\epsilon}$$\n\nTherefore, passing a dual number into our generic function, with $b = 1$, will return, with minimal overhead, both the function value and the derivative at $a$. This is the basis of forward mode automatic differentiation.\n\nIt is important to note that this is **not** an approximation of the derivative by finite differencing. Finite differencing, will estimate the derivative with two function calls:\n\n$$f'(a) \\approxeq \\frac{f(a + h) - f(a)}{h}$$\n\nThe problem with this approach is that using too large a value for $h$, give inaccuracies in the estimation, but using too small a value of $h$ give inaccuracies through floating point rounding errors. There is therefore an optimal value of $h$, which will depend on the value of the function at the point of evaluation. This value is generally both unknown and unknowable. Automatic differentiation does not have this problem and also has fewer function calls, so is more efficient as well as more accurate.\n\nWe can also easily expand the method to multiple dimensions by simply expanding the idea of a dual number:\n\n$$z = a + {\\epsilon}_1v_1 + {\\epsilon}_2v_2 + {\\epsilon}_3v_3 + ...$$\n\nwhere each $v$ is a basis vector in which direction we wish to calculate a derivative.\n\n$$f(z) = f(a) + f'(a)v_1{\\epsilon}_1+ f'(a)v_1{\\epsilon}_1+ f'(a)v_1{\\epsilon}_1$$\n\nThis works perfectly well for a simple analytic function. To extend this to more complicated functions, we simply apply the rules for differentiation:\n\n$$c = a + b   =>  dc = da + db$$ $$c = a - b   =>  dc = da - db$$ $$c = a * b   =>  dc = b*da + a*db$$ $$dc = a / b    =>  dc = (da*b - a*db)/b^2$$ $$c = sin(a)    =>  dc = cos(a)*da$$ $$\\text{etc.}$$\n\nThis is implemented by providing Julia with operator overloading for the dual numbers, i.e. telling Julia how to add, subtract etc. with dual numbers.\n\nAs a simple example, we define a dual number type and tell Julia how to add and divide dual numbers, so we can test it with the Babylonian algorithm for calculating square roots. This is an example given by Prof Alan Edelman, one of the designers of the Julia language in a [YouTube](https://www.youtube.com/watch?v=vAp6nUMrKYg) video, that is well worth watching.\n\n``` julia\n# Import the Base functions we are going to extend\nimport Base: +, /, convert, promote_rule\n\n# Define our Dual number type as a sub-type of Number\nstruct Dual <: Number\n    f::Tuple{Float64, Float64}\nend\n\n# Tell Julia how to add and divide with Dual\n+(x::Dual, y::Dual) = Dual(x.f .+ y.f)\n/(x::Dual, y::Dual) = Dual((x.f[1]/y.f[1], (y.f[1]*x.f[2] - x.f[1]*y.f[2])/y.f[1]^2))\n\n# Tell Julia how to deal with type conversions\nconvert(::Type{Dual}, x::Real) = Dual((float(x), 0.0))\npromote_rule(::Type{Dual}, ::Type{<:Number}) = Dual\n\n\"\"\"\nThe Babylonian algorithm for square roots\n-----------------------------------------\nInitial guess is 1, then for each iteration, take the average of the current estimate, a,\nand x/a. If the current estimate is too large, x/a will be too small and vice versa. \nTaking the average of the two brings us closer with each step. Usually, ten iterations is\nmore than enough.\n\"\"\"\nfunction babylonian(x, n = 10)\n    a = (x + 1)/2\n    for i in 2:n\n        a = (a + x/a)/2\n    end\n    return a\nend\n\n# Test our algorithm\nbabylonian(2) # 1.414213562373095\nerr = babylonian(2) - √2 # -2.220446049250313e-16\n\n# Create a dual number, with the epsilon part set to 1, to \"seed\" the derivative for the first variable\nd = Dual((2., 1.)) # Dual((2.0, 1.0))\n\n# Run the same code we just used for real numbers\nres = babylonian(d) # Dual((1.414213562373095, 0.35355339059327373))\n\n# Check the results\nres.f[1] - √2 # -2.220446049250313e-16\nres.f[2] - 0.5/√2 # 0.0\n```\n\nHere, we have defined the conversion of a constant to a `Dual` by setting the derivative part to zero, since the derivative of a constant is zero.\n\nWhen we call the function, however, we set the $b$ (from $z = a + {\\epsilon}b$) to *one*, since we want to calculate the derivative with regards to this variable. If this was a multivariate function, we would pick which derivative we wanted by which of the $b$ values we set to one, with the others set to zero.\n\n## Reverse Mode AD\n\nForward mode AD is fairly easy to understand and implement, but it comes with a disadvantage. If we had $n$ input variables, we would need $n$ function calls to calculate the full gradient.\n\nWhen fitting models to data, we usually do not care about the derivative in terms of the input, but rather in terms of the parameters of the model, of which there can be many. We can apply automatic differentiation here as well, by passing the parameters as dual numbers. For $k$ parameters, we have a $k+1$ dimensional \"dual\" number and $k$ function calls.\n\nWhen there are many parameters, it is possible to use a more efficient method than forward mode automatic differentiation.\n\nConsider fitting a model. Our model has some outputs, $w_i$, and some inputs, $u_i$. We can use the chain rule to write the derivative of output $w$ in terms of some yet-to-be-chosen variable, $t$:\n\n$$\\frac{{\\partial}w}{{\\partial}t} = \\sum_i{\\frac{{\\partial}w}{{\\partial}u_i} \\frac{{\\partial}u_i}{{\\partial}t}}$$\n\nTo get the derivative in terms of variable $x$, we simply set $t = x$.\n\nThe chain rule is symmetric - we can change the numerators and denominators around:\n\n$$\\frac{{\\partial}s}{{\\partial}u} = \\sum_i{\\frac{{\\partial}w_i}{{\\partial}u} \\frac{{\\partial}s}{{\\partial}w_i}}$$\n\nWhere $u$ is still an input, $w$ an output, and $s$ some yet-to-be-chosen variable. This time, however, we are calculating the derivative of $s$ with regards to the input, $u$. We have now reversed the positions: we are calculating the derivative of $s$ with regards to input $u$.\n\nLet's look at a simple example:\n\n$$z = xy + sin(x)$$\n\nWe can define intermediate variables:\n\n$$a = xy$$ $$b = sin(x)$$ $$z = a + b$$\n\nUsing the reverse chain-rule from above:\n\n$$\\frac{\\partial s}{\\partial b} = \\frac{\\partial z}{\\partial b} \\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial z}$$\n\n$$\\frac{\\partial s}{\\partial a} = \\frac{\\partial z}{\\partial a} \\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial z}$$\n\n$$\\frac{\\partial s}{\\partial y} = \\frac{\\partial a}{\\partial y} \\frac{\\partial s}{\\partial a} = x \\frac{\\partial s}{\\partial a}$$\n\n$$\\frac{\\partial s}{\\partial x} = \\frac{\\partial a}{\\partial x} \\frac{\\partial s}{\\partial a} + \\frac{\\partial b}{\\partial x} \\frac{\\partial s}{\\partial b} = y \\frac{\\partial s}{\\partial a} + cos(x) \\frac{\\partial s}{\\partial b}$$\n\nIf we now substitute $s = z$, we get:\n\n$$\\frac{\\partial z}{\\partial b} = \\frac{\\partial z}{\\partial z} = 1$$\n\n$$\\frac{\\partial z}{\\partial a} = \\frac{\\partial z}{\\partial z} = 1$$\n\n$$\\frac{\\partial z}{\\partial y} = x \\frac{\\partial z}{\\partial a} = x$$\n\n$$\\frac{\\partial z}{\\partial x} = y \\frac{\\partial z}{\\partial a} + cos(x) \\frac{\\partial z}{\\partial b} = y + cos(x)$$\n\nWe have now managed to calculate both derivatives in a single pass! When training a neural network, that can have hundreds (small ANN) to billions of parameters (ChatGPT), being able to get the full gradient of the model output with regards to all of the model parameters in a single pass is a dramatic improvement in efficiency.\n\nIn general, when a model has many more outputs than inputs, forward mode automatic differentiation will be the more efficient method and when there are many more inputs than outputs, such as in machine learning, reverse mode will be more efficient. If your model is not large, or not called many times (such as when fitting parameters), you may not notice the difference.\n\n## Available packages\n\n### ForwardDiff.jl\n\nOne of the most used automatic differentiation packages in Julia is [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl). It has a few limitations to its use:\n\n1.  It only works on Julia code - you cannot use it on C, Python or R code being called from Julia\n2.  Target functions must take only one input, which may be a vector, and return either a scalar or vector\n3.  The function must be written generically, i.e. no types must be specified. This is something that can quickly trip you up, when you for example, initialise a variable to 0.0 or 1.0, both of which are `Float64`. Rather use `zero(x)` and `one(x)`, which will return a zero or one in the same type as `x`.\n4.  Input vectors must be a sub-type of `AbstractArray`, so some custom types may not be supported. Julia built-in arrays and `StaticArrays` are supported.\n\nThe package provides several functions, including:\n\n`ForwardDiff.derivative(f, x::Real)`: Calculate the derivative of `f(x)` in variable `x` at the current value of `x`\n\n`ForwardDiff.gradient(f, x::AbstractArray)`: Calculate the gradient of `f(x)` in each of the variables contained in vector `x` at the current value of `x`\n\n`ForwardDiff.jacobian(f, x::AbstractArray)`: Calculate the Jacobian matrix, where `f(x)` returns a vector of function values calculated from input vector `x`.\n\n`ForwardDiff.hessian(f, x::AbstractArray)`: Calculate the Hessian matrix, where f returns a scalar value, calculated from input vector `x`.\n\nA simple example:\n\n``` julia\nf(x) = 2x^2 - x\n# f (generic function with 1 method)\n\ndf(x) = ForwardDiff.derivative(f, x)\n# df (generic function with 1 method)\n\nf(2)\n# 6\n\ndf(2)\n# 7\n```\n\nForwardDiff does have the option to handle mutating functions, where the array result is returned in the first parameter. Several packages use ForwardDiff as an option integrated in solvers, including Roots.jl and Optim.jl, and some of the implicit differential equation solvers.\n\n### Zygote.jl\n\nThe [Zygote](https://fluxml.ai/Zygote.jl/latest/) package was developed as part of [Flux.j](http://fluxml.ai/) - a deep learning package in Julia.\n\nZygote cannot handle mutating functions, try/catch statements for exception handling or calls to \"foreign\" code, like a C library. It works during the compilation phase, so generates very efficient code, using reverse mode automatic differentiation.\n\nUsing Zygote is fairly similar to using ForwardDiff.\n\n`gradient()` - calculates the derivative of a function, with either scalar or vector inputs\n\n`jacobian()` - calculates the Jacobian matrix for a system of equations, as well as hessian\n\n`hessian()` - calculates the Hessian matrix for an equation with vector of inputs\n\nThere are also some additional utility functions. See the [documentation](https://fluxml.ai/Zygote.jl/latest/utils/) for details.\n\n### Enzyme.jl\n\n[Enzyme](https://enzyme.mit.edu/index.fcgi/julia/stable/) is the future of automatic differentiation in Julia. It is a point of debate whether it is currently ready for general use, but if you use AD in your code, it will be good to keep an eye on this package.\n\nForwardDiff works by passing dual numbers to your code and using the [ChainRules.jl](https://github.com/JuliaDiff/ChainRules.jl) package to supply the rules for differentiating more complex equations. Zygote goes a level deeper and generates the code for the derivatives during the compilation pass, using a process of source code transformation. The curious / adventurous can read the [article](https://arxiv.org/pdf/1810.07951.pdf) for details.\n\nEnzyme goes another level deeper and works on the LLVM code generated by the Julia compiler. The result is that there are few, if any, limitations on the code that can be differentiated and the resulting code is very, very fast. Enzyme itself is still under development. It is currently available in Julia and Rust. Enzyme can perform both forward and reverse mode differentiation.\n\nThere are several exported functions, but the ones most likely to be useful are:\n\n`gradient()`: Calculates the gradient of am array-input function\n\n`gradient!()`: Calculates the gradient of am array-input function and returns it by updating a passed array variable\n\n`jacobian()`: calculates the Jacobian matrix of a set of equations\n\nFor all of these, the mode (forward or reverse) can be specified.\n\nWatch this space!","srcMarkdownNoYaml":"\n\n## Dual Numbers - Forward Mode AD\n\nAny reader of this document will be familiar with complex numbers:\n\n$$z = x + iy$$ where $i^2 = -1$\n\nThe concept of [dual numbers](https://en.wikipedia.org/wiki/Dual_number), proposed by Clifford, in 1873, is closely related:\n\n$$z = a + {\\epsilon}b$$ where ${\\epsilon}^2 = 0, \\epsilon \\ne 0$\n\nIt can be proven that for any analytic[^1] function,\n\n[^1]: Any function that is locally given by a convergent power series, such as a Taylor series\n\n$$f(a + {\\epsilon}b) = f(a) + bf'(a){\\epsilon}$$\n\nTherefore, passing a dual number into our generic function, with $b = 1$, will return, with minimal overhead, both the function value and the derivative at $a$. This is the basis of forward mode automatic differentiation.\n\nIt is important to note that this is **not** an approximation of the derivative by finite differencing. Finite differencing, will estimate the derivative with two function calls:\n\n$$f'(a) \\approxeq \\frac{f(a + h) - f(a)}{h}$$\n\nThe problem with this approach is that using too large a value for $h$, give inaccuracies in the estimation, but using too small a value of $h$ give inaccuracies through floating point rounding errors. There is therefore an optimal value of $h$, which will depend on the value of the function at the point of evaluation. This value is generally both unknown and unknowable. Automatic differentiation does not have this problem and also has fewer function calls, so is more efficient as well as more accurate.\n\nWe can also easily expand the method to multiple dimensions by simply expanding the idea of a dual number:\n\n$$z = a + {\\epsilon}_1v_1 + {\\epsilon}_2v_2 + {\\epsilon}_3v_3 + ...$$\n\nwhere each $v$ is a basis vector in which direction we wish to calculate a derivative.\n\n$$f(z) = f(a) + f'(a)v_1{\\epsilon}_1+ f'(a)v_1{\\epsilon}_1+ f'(a)v_1{\\epsilon}_1$$\n\nThis works perfectly well for a simple analytic function. To extend this to more complicated functions, we simply apply the rules for differentiation:\n\n$$c = a + b   =>  dc = da + db$$ $$c = a - b   =>  dc = da - db$$ $$c = a * b   =>  dc = b*da + a*db$$ $$dc = a / b    =>  dc = (da*b - a*db)/b^2$$ $$c = sin(a)    =>  dc = cos(a)*da$$ $$\\text{etc.}$$\n\nThis is implemented by providing Julia with operator overloading for the dual numbers, i.e. telling Julia how to add, subtract etc. with dual numbers.\n\nAs a simple example, we define a dual number type and tell Julia how to add and divide dual numbers, so we can test it with the Babylonian algorithm for calculating square roots. This is an example given by Prof Alan Edelman, one of the designers of the Julia language in a [YouTube](https://www.youtube.com/watch?v=vAp6nUMrKYg) video, that is well worth watching.\n\n``` julia\n# Import the Base functions we are going to extend\nimport Base: +, /, convert, promote_rule\n\n# Define our Dual number type as a sub-type of Number\nstruct Dual <: Number\n    f::Tuple{Float64, Float64}\nend\n\n# Tell Julia how to add and divide with Dual\n+(x::Dual, y::Dual) = Dual(x.f .+ y.f)\n/(x::Dual, y::Dual) = Dual((x.f[1]/y.f[1], (y.f[1]*x.f[2] - x.f[1]*y.f[2])/y.f[1]^2))\n\n# Tell Julia how to deal with type conversions\nconvert(::Type{Dual}, x::Real) = Dual((float(x), 0.0))\npromote_rule(::Type{Dual}, ::Type{<:Number}) = Dual\n\n\"\"\"\nThe Babylonian algorithm for square roots\n-----------------------------------------\nInitial guess is 1, then for each iteration, take the average of the current estimate, a,\nand x/a. If the current estimate is too large, x/a will be too small and vice versa. \nTaking the average of the two brings us closer with each step. Usually, ten iterations is\nmore than enough.\n\"\"\"\nfunction babylonian(x, n = 10)\n    a = (x + 1)/2\n    for i in 2:n\n        a = (a + x/a)/2\n    end\n    return a\nend\n\n# Test our algorithm\nbabylonian(2) # 1.414213562373095\nerr = babylonian(2) - √2 # -2.220446049250313e-16\n\n# Create a dual number, with the epsilon part set to 1, to \"seed\" the derivative for the first variable\nd = Dual((2., 1.)) # Dual((2.0, 1.0))\n\n# Run the same code we just used for real numbers\nres = babylonian(d) # Dual((1.414213562373095, 0.35355339059327373))\n\n# Check the results\nres.f[1] - √2 # -2.220446049250313e-16\nres.f[2] - 0.5/√2 # 0.0\n```\n\nHere, we have defined the conversion of a constant to a `Dual` by setting the derivative part to zero, since the derivative of a constant is zero.\n\nWhen we call the function, however, we set the $b$ (from $z = a + {\\epsilon}b$) to *one*, since we want to calculate the derivative with regards to this variable. If this was a multivariate function, we would pick which derivative we wanted by which of the $b$ values we set to one, with the others set to zero.\n\n## Reverse Mode AD\n\nForward mode AD is fairly easy to understand and implement, but it comes with a disadvantage. If we had $n$ input variables, we would need $n$ function calls to calculate the full gradient.\n\nWhen fitting models to data, we usually do not care about the derivative in terms of the input, but rather in terms of the parameters of the model, of which there can be many. We can apply automatic differentiation here as well, by passing the parameters as dual numbers. For $k$ parameters, we have a $k+1$ dimensional \"dual\" number and $k$ function calls.\n\nWhen there are many parameters, it is possible to use a more efficient method than forward mode automatic differentiation.\n\nConsider fitting a model. Our model has some outputs, $w_i$, and some inputs, $u_i$. We can use the chain rule to write the derivative of output $w$ in terms of some yet-to-be-chosen variable, $t$:\n\n$$\\frac{{\\partial}w}{{\\partial}t} = \\sum_i{\\frac{{\\partial}w}{{\\partial}u_i} \\frac{{\\partial}u_i}{{\\partial}t}}$$\n\nTo get the derivative in terms of variable $x$, we simply set $t = x$.\n\nThe chain rule is symmetric - we can change the numerators and denominators around:\n\n$$\\frac{{\\partial}s}{{\\partial}u} = \\sum_i{\\frac{{\\partial}w_i}{{\\partial}u} \\frac{{\\partial}s}{{\\partial}w_i}}$$\n\nWhere $u$ is still an input, $w$ an output, and $s$ some yet-to-be-chosen variable. This time, however, we are calculating the derivative of $s$ with regards to the input, $u$. We have now reversed the positions: we are calculating the derivative of $s$ with regards to input $u$.\n\nLet's look at a simple example:\n\n$$z = xy + sin(x)$$\n\nWe can define intermediate variables:\n\n$$a = xy$$ $$b = sin(x)$$ $$z = a + b$$\n\nUsing the reverse chain-rule from above:\n\n$$\\frac{\\partial s}{\\partial b} = \\frac{\\partial z}{\\partial b} \\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial z}$$\n\n$$\\frac{\\partial s}{\\partial a} = \\frac{\\partial z}{\\partial a} \\frac{\\partial s}{\\partial z} = \\frac{\\partial s}{\\partial z}$$\n\n$$\\frac{\\partial s}{\\partial y} = \\frac{\\partial a}{\\partial y} \\frac{\\partial s}{\\partial a} = x \\frac{\\partial s}{\\partial a}$$\n\n$$\\frac{\\partial s}{\\partial x} = \\frac{\\partial a}{\\partial x} \\frac{\\partial s}{\\partial a} + \\frac{\\partial b}{\\partial x} \\frac{\\partial s}{\\partial b} = y \\frac{\\partial s}{\\partial a} + cos(x) \\frac{\\partial s}{\\partial b}$$\n\nIf we now substitute $s = z$, we get:\n\n$$\\frac{\\partial z}{\\partial b} = \\frac{\\partial z}{\\partial z} = 1$$\n\n$$\\frac{\\partial z}{\\partial a} = \\frac{\\partial z}{\\partial z} = 1$$\n\n$$\\frac{\\partial z}{\\partial y} = x \\frac{\\partial z}{\\partial a} = x$$\n\n$$\\frac{\\partial z}{\\partial x} = y \\frac{\\partial z}{\\partial a} + cos(x) \\frac{\\partial z}{\\partial b} = y + cos(x)$$\n\nWe have now managed to calculate both derivatives in a single pass! When training a neural network, that can have hundreds (small ANN) to billions of parameters (ChatGPT), being able to get the full gradient of the model output with regards to all of the model parameters in a single pass is a dramatic improvement in efficiency.\n\nIn general, when a model has many more outputs than inputs, forward mode automatic differentiation will be the more efficient method and when there are many more inputs than outputs, such as in machine learning, reverse mode will be more efficient. If your model is not large, or not called many times (such as when fitting parameters), you may not notice the difference.\n\n## Available packages\n\n### ForwardDiff.jl\n\nOne of the most used automatic differentiation packages in Julia is [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl). It has a few limitations to its use:\n\n1.  It only works on Julia code - you cannot use it on C, Python or R code being called from Julia\n2.  Target functions must take only one input, which may be a vector, and return either a scalar or vector\n3.  The function must be written generically, i.e. no types must be specified. This is something that can quickly trip you up, when you for example, initialise a variable to 0.0 or 1.0, both of which are `Float64`. Rather use `zero(x)` and `one(x)`, which will return a zero or one in the same type as `x`.\n4.  Input vectors must be a sub-type of `AbstractArray`, so some custom types may not be supported. Julia built-in arrays and `StaticArrays` are supported.\n\nThe package provides several functions, including:\n\n`ForwardDiff.derivative(f, x::Real)`: Calculate the derivative of `f(x)` in variable `x` at the current value of `x`\n\n`ForwardDiff.gradient(f, x::AbstractArray)`: Calculate the gradient of `f(x)` in each of the variables contained in vector `x` at the current value of `x`\n\n`ForwardDiff.jacobian(f, x::AbstractArray)`: Calculate the Jacobian matrix, where `f(x)` returns a vector of function values calculated from input vector `x`.\n\n`ForwardDiff.hessian(f, x::AbstractArray)`: Calculate the Hessian matrix, where f returns a scalar value, calculated from input vector `x`.\n\nA simple example:\n\n``` julia\nf(x) = 2x^2 - x\n# f (generic function with 1 method)\n\ndf(x) = ForwardDiff.derivative(f, x)\n# df (generic function with 1 method)\n\nf(2)\n# 6\n\ndf(2)\n# 7\n```\n\nForwardDiff does have the option to handle mutating functions, where the array result is returned in the first parameter. Several packages use ForwardDiff as an option integrated in solvers, including Roots.jl and Optim.jl, and some of the implicit differential equation solvers.\n\n### Zygote.jl\n\nThe [Zygote](https://fluxml.ai/Zygote.jl/latest/) package was developed as part of [Flux.j](http://fluxml.ai/) - a deep learning package in Julia.\n\nZygote cannot handle mutating functions, try/catch statements for exception handling or calls to \"foreign\" code, like a C library. It works during the compilation phase, so generates very efficient code, using reverse mode automatic differentiation.\n\nUsing Zygote is fairly similar to using ForwardDiff.\n\n`gradient()` - calculates the derivative of a function, with either scalar or vector inputs\n\n`jacobian()` - calculates the Jacobian matrix for a system of equations, as well as hessian\n\n`hessian()` - calculates the Hessian matrix for an equation with vector of inputs\n\nThere are also some additional utility functions. See the [documentation](https://fluxml.ai/Zygote.jl/latest/utils/) for details.\n\n### Enzyme.jl\n\n[Enzyme](https://enzyme.mit.edu/index.fcgi/julia/stable/) is the future of automatic differentiation in Julia. It is a point of debate whether it is currently ready for general use, but if you use AD in your code, it will be good to keep an eye on this package.\n\nForwardDiff works by passing dual numbers to your code and using the [ChainRules.jl](https://github.com/JuliaDiff/ChainRules.jl) package to supply the rules for differentiating more complex equations. Zygote goes a level deeper and generates the code for the derivatives during the compilation pass, using a process of source code transformation. The curious / adventurous can read the [article](https://arxiv.org/pdf/1810.07951.pdf) for details.\n\nEnzyme goes another level deeper and works on the LLVM code generated by the Julia compiler. The result is that there are few, if any, limitations on the code that can be differentiated and the resulting code is very, very fast. Enzyme itself is still under development. It is currently available in Julia and Rust. Enzyme can perform both forward and reverse mode differentiation.\n\nThere are several exported functions, but the ones most likely to be useful are:\n\n`gradient()`: Calculates the gradient of am array-input function\n\n`gradient!()`: Calculates the gradient of am array-input function and returns it by updating a passed array variable\n\n`jacobian()`: calculates the Jacobian matrix of a set of equations\n\nFor all of these, the mode (forward or reverse) can be specified.\n\nWatch this space!"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"embed-resources":true,"output-file":"10_Automatic_Differentiation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.361","theme":"cosmo","monofont":"JetBrains Mono","smooth-scroll":false,"footnotes-hover":true,"title":"Automatic Differentiation"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}