{"title":"Optimisation","markdown":{"yaml":{"title":"Optimisation"},"headingText":"Available packages","containsRefs":false,"markdown":"\n\n\nThere are many optimisation packages available in Julia, some for general purposes, other for very specific applications. The most commonly used ones include [Optim](https://github.com/JuliaNLSolvers/Optim.jl), [NLopt](https://github.com/JuliaOpt/NLopt.jl), [JuMP](https://jump.dev/), [Convex](https://github.com/jump-dev/Convex.jl), [BlackBoxOptim](https://github.com/robertfeldt/BlackBoxOptim.jl) and [IntervalOptimisation](https://juliaintervals.github.io/pages/packages/intervaloptimisation/).\n\nOptim and NLopt include several local optimisers. This include gradient free methods, such as Nelder-Mead, Particle Swarm and Simulated Annealing. There are gradient-based methods, such as Conjugated Gradient, Gradient Descent and BFGS/LBFGS, and finally methods that require the Hessian, like Newton's method.\n\nJuMP and Convex are powerful packages for mathematical programming - linear and non-linear optimisation with linear and/or non-linear constraints. These packages do not include solvers, but allow you to specify the problem and then call one of several external solvers that have to installed separately. This includes both open source and commercial solvers.\n\nBlackBoxOptim contains a variety of global optimisation algorithms (within the specified bounded area). These are non-gradient methods like Evolution Strategy methods and stochastic searches.\n\nWhich package you use will depend on the type of problem you are solving and your personal preferences. For typical local optimisations in process engineering, Optim is very suitable. If you are doing a plant-wide optimisation, JuMP is a very powerful tool. If the function your are optimising has many local minima, BlackBoxOptim and IntervalOptimisation are both very robust. IntervalOptimisation also comes with mathematical guarantees of finding the global optimum in the search area.\n\nUnfortunately, this also implies that you would need to learn how to use several packages. This is were the SciML organisation again is very helpful. Their [Optimization](https://docs.sciml.ai/Optimization/stable/)[^1] package provides a unified front-end for several other optimisation packages, including Optim, NLopt, BlackBoxOptim and all of the external solvers available to JuMP and Convex. There full [list](https://docs.sciml.ai/Optimization/stable/#Overview-of-the-Optimizers) includes several others as well. Optimizers can also be used with Flux - a deep learning package - to train neural networks.\n\n[^1]: This package was originally called GalacticOptim to indicate that it includes a galaxy of optimisers, but the name was shortened due to popular demand.\n\nAs with the other sections in this tutorial, we shall look only at the unified front-end of Optimisation. For each of the other packages, the documentation linked to above will provide information on how to use the package and several examples.\n\n## Optimization.jl\n\nTo use the various optimiser packages with Optimization, you must also install and use packages that link the external solvers with the Optimization interface. For example, to use the solvers in Optim, you would need to use the Optimization package along with the OptimizationOptimJL package, and to use BlackBoxOptim, you would use the OptimizationBBO package along with Optimization. These additional packages are used in order to prevent the need for having to install **all** of the solver packages as dependencies for Optimization. The specific linker packages needed are given in the documentation section for *Optimizer Packages*.\n\nFor our example, we use the famous [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function). This function is notorious for pushing optimisers to their limit due to its large flat section around the optimum at (1.0, 1.0), as well as steep slopes away from the optimum.\n\n$$f(x, y) = (a - x)^2 + b(y - x^2)^2$$ with $a = 1$ and $b = 100$\n\n![Rosenbrock's function in two dimensions](/img/rosebrock.svg)\n\nIn this example (taken from the introductory tutorials of Optimization.jl), we use two optimisers, Nelder-Mead from Optim and an Adaptive Differential Evolution method from BlackBoxOptim. We also time both of these solvers using `@btime` from BenchmarkTools.\n\n``` julia\nusing Optimization, OptimizationOptimJL, OptimizationBBO, BenchmarkTools\n\nrosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\nu0 = zeros(2)\np = [1.0, 100.0]\n\n# Use NelderMead from Optim.jl, linked in via OptimizationOptimJL\n\nprob = OptimizationProblem(rosenbrock, u0, p)\n@btime sol = solve(prob, NelderMead())\n\n# 945.400 μs (3309 allocations: 168.98 KiB)\n# u: 2-element Vector{Float64}:\n#  0.9999634355313174\n#  0.9999315506115275\n\n# Use Adaptive Differential Evolution method adaptive_de_rand_1_bin_radiuslimited from\n# BlackBoxOptim.jl, via OptimizationBBO. This method needs a bounded search area.\n\nprob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())\n\n# 13.260 ms (153718 allocations: 5.69 MiB)\n# u: 2-element Vector{Float64}:\n#  0.9999999999999495\n#  0.9999999999998918\n```\n\nAs we see, both methods could find the optimum for this function, although the genetic algorithm method from BlackBoxOptim took about 14x longer. This is typical for genetic algorithm methods. They are not fast, but they are extremely robust and will often still find the optima where other methods will fail.\n\nThe benefit of using Optimization.jl as a front-end is indeed in these cases of difficult to optimise functions, since you can easily change algorithms to find the best methods for your specific problem.","srcMarkdownNoYaml":"\n\n## Available packages\n\nThere are many optimisation packages available in Julia, some for general purposes, other for very specific applications. The most commonly used ones include [Optim](https://github.com/JuliaNLSolvers/Optim.jl), [NLopt](https://github.com/JuliaOpt/NLopt.jl), [JuMP](https://jump.dev/), [Convex](https://github.com/jump-dev/Convex.jl), [BlackBoxOptim](https://github.com/robertfeldt/BlackBoxOptim.jl) and [IntervalOptimisation](https://juliaintervals.github.io/pages/packages/intervaloptimisation/).\n\nOptim and NLopt include several local optimisers. This include gradient free methods, such as Nelder-Mead, Particle Swarm and Simulated Annealing. There are gradient-based methods, such as Conjugated Gradient, Gradient Descent and BFGS/LBFGS, and finally methods that require the Hessian, like Newton's method.\n\nJuMP and Convex are powerful packages for mathematical programming - linear and non-linear optimisation with linear and/or non-linear constraints. These packages do not include solvers, but allow you to specify the problem and then call one of several external solvers that have to installed separately. This includes both open source and commercial solvers.\n\nBlackBoxOptim contains a variety of global optimisation algorithms (within the specified bounded area). These are non-gradient methods like Evolution Strategy methods and stochastic searches.\n\nWhich package you use will depend on the type of problem you are solving and your personal preferences. For typical local optimisations in process engineering, Optim is very suitable. If you are doing a plant-wide optimisation, JuMP is a very powerful tool. If the function your are optimising has many local minima, BlackBoxOptim and IntervalOptimisation are both very robust. IntervalOptimisation also comes with mathematical guarantees of finding the global optimum in the search area.\n\nUnfortunately, this also implies that you would need to learn how to use several packages. This is were the SciML organisation again is very helpful. Their [Optimization](https://docs.sciml.ai/Optimization/stable/)[^1] package provides a unified front-end for several other optimisation packages, including Optim, NLopt, BlackBoxOptim and all of the external solvers available to JuMP and Convex. There full [list](https://docs.sciml.ai/Optimization/stable/#Overview-of-the-Optimizers) includes several others as well. Optimizers can also be used with Flux - a deep learning package - to train neural networks.\n\n[^1]: This package was originally called GalacticOptim to indicate that it includes a galaxy of optimisers, but the name was shortened due to popular demand.\n\nAs with the other sections in this tutorial, we shall look only at the unified front-end of Optimisation. For each of the other packages, the documentation linked to above will provide information on how to use the package and several examples.\n\n## Optimization.jl\n\nTo use the various optimiser packages with Optimization, you must also install and use packages that link the external solvers with the Optimization interface. For example, to use the solvers in Optim, you would need to use the Optimization package along with the OptimizationOptimJL package, and to use BlackBoxOptim, you would use the OptimizationBBO package along with Optimization. These additional packages are used in order to prevent the need for having to install **all** of the solver packages as dependencies for Optimization. The specific linker packages needed are given in the documentation section for *Optimizer Packages*.\n\nFor our example, we use the famous [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function). This function is notorious for pushing optimisers to their limit due to its large flat section around the optimum at (1.0, 1.0), as well as steep slopes away from the optimum.\n\n$$f(x, y) = (a - x)^2 + b(y - x^2)^2$$ with $a = 1$ and $b = 100$\n\n![Rosenbrock's function in two dimensions](/img/rosebrock.svg)\n\nIn this example (taken from the introductory tutorials of Optimization.jl), we use two optimisers, Nelder-Mead from Optim and an Adaptive Differential Evolution method from BlackBoxOptim. We also time both of these solvers using `@btime` from BenchmarkTools.\n\n``` julia\nusing Optimization, OptimizationOptimJL, OptimizationBBO, BenchmarkTools\n\nrosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\nu0 = zeros(2)\np = [1.0, 100.0]\n\n# Use NelderMead from Optim.jl, linked in via OptimizationOptimJL\n\nprob = OptimizationProblem(rosenbrock, u0, p)\n@btime sol = solve(prob, NelderMead())\n\n# 945.400 μs (3309 allocations: 168.98 KiB)\n# u: 2-element Vector{Float64}:\n#  0.9999634355313174\n#  0.9999315506115275\n\n# Use Adaptive Differential Evolution method adaptive_de_rand_1_bin_radiuslimited from\n# BlackBoxOptim.jl, via OptimizationBBO. This method needs a bounded search area.\n\nprob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())\n\n# 13.260 ms (153718 allocations: 5.69 MiB)\n# u: 2-element Vector{Float64}:\n#  0.9999999999999495\n#  0.9999999999998918\n```\n\nAs we see, both methods could find the optimum for this function, although the genetic algorithm method from BlackBoxOptim took about 14x longer. This is typical for genetic algorithm methods. They are not fast, but they are extremely robust and will often still find the optima where other methods will fail.\n\nThe benefit of using Optimization.jl as a front-end is indeed in these cases of difficult to optimise functions, since you can easily change algorithms to find the best methods for your specific problem."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"embed-resources":true,"output-file":"09_Optimisation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.361","theme":"cosmo","monofont":"JetBrains Mono","smooth-scroll":false,"footnotes-hover":true,"title":"Optimisation"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}